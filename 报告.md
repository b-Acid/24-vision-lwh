![img](https://i0.hdslb.com/bfs/new_dyn/791944995fff725f42c7f5a9b64f8567100423098.png@1295w.webp)

# 华南虎视觉组实习生任务

实习生姓名：刘玮涵

## 一、公共任务　

### 装甲板识别

#### 1. 代码思路
1. 对图像进行通道分离，分为BGR三个通道。
2. 按探测颜色对对应通道的图像进行二值化。
3. 对图像进行高斯滤波，减少噪点。
4. 用cv::dilate()进行膨胀，使轮廓边缘圆滑。
5. 使用漫水法使图像平滑。
6. 调用cv::findContours()进行轮廓检测。
7. 对每个轮廓做最小外接矩形，筛除不符合灯柱形状要求的矩形。轮廓要进一步简化为线段，取得到的矩形的较短两边灯柱两个中心作为轮廓的两个端点。
8. 对所有轮廓两两匹配，按照预设参数匹配两个轮廓是否组成装甲板。
> 这里的预设参数有：灯柱长宽比，装甲板长宽比，灯柱角度差，灯柱长度差，装甲板四边形倾斜程度（此处对装甲板是按两个灯柱组成类似梯形来判定的，倾斜程度表示梯形两条边平行防线上距离）
10. 标记匹配成功的装甲板，用pnp算法，cv::solvepnp()结算其相对相机的坐标，即可得到位姿。

#### 2. 遇到问题和解决思路
> 1. <font color='red'>探测到的灯管长方形四个点无法定序。灯管的1~4号端点的顺序在opencv里默认是以离原点最近的为1号然后顺时针一圈排列四个点，但是离原点最近的那个点因为装甲板的方位会比较随机，导致4个点无法定序。 </font>



__解决方式__：   
+ 在灯管类的初始化时分别计算灯管长方形的四条边的中心和整个长方形的中心坐标，然后计算哪两个边中心离形心更远，就拿这两个边中心作为轮廓线段的端点。y坐标小的为0号端点，y坐标大的为1号端点。虽然这个方法在装甲板是竖着的时候不适用，但99%的识别任务中装甲板是横的。这样处理后就不用管点的定序问题了，而且识别到的装甲板永远以左灯柱的上端点为1号端点然后顺时针一周排布4个端点。

> 2. <font color='red'>实测发现最小外接矩形法很不稳定，因为它真的就是找最小外接矩形，套出来的外接矩形会因为图形边缘的不稳定左右晃动</font>

__解决方式__：  
+ 改进为把轮廓作为椭圆形轮廓，利用cv::fitEllipse()来给灯管矩形

> 3. <font color='red'>虽然cv::findContours()的输出轮廓是无序的，但是轮廓两两匹配确实简单粗暴，比较耗时</font>。

__解决方式__：  
+ 改进方案为对每个轮廓按x坐标排序，每个轮廓只和左右三个轮廓匹配一次  。

+ _10.05记：对每个轮廓排序也是个不小时间开支，因为排序涉及到对内存区域交换、开新空间的操作，两两匹配可能是最优的方法了_。


#### 3. 效果图
+ ![装甲板效果图](https://github.com/b-Acid/24-vision-lwh/blob/main/%E8%A3%85%E7%94%B2%E6%9D%BF%E8%AF%86%E5%88%AB/output.png?raw=true)
+ 效果图中四个角点并没有打在灯柱上，这是因为上面提出的问题2中把最小矩形改成了cv::fitEllipse()。当然这影响不大，因为拟合出的长方形的长和灯柱实际的长的像素是成比例的，可以后期通过调参实现精准测量位姿。

#### 4. 总结
+ 本任务中我熟悉了opencv中一些对图像的基本处理方法，比如Mat类的基本操作，图像通道分离，二值化图像，高斯滤波，图形膨胀，漫水法，轮廓检测等等。另外呢学到了pnp算法，了解了二位像素坐标和世界坐标的转换关系，实现了基于拍摄图和实物参数结算相机姿态的操作。





## 二、专属方向（此处填写专属方向）任务
### 1.CMake任务
#### 思路
+ 首先将common和modules里的cpp文件编译成两个静态库common和modules，然后在根目录里的CMakeLists.txt里链接这两个库。
+ 找到opencv的路径，链接opencv动态库。

#### 使用方法
+ 依次输入以下命令完成编译：
```
mkdir build
cd build
cmake ..
make -j8
./test

```

#### 最终运行效果

```
M1 construct
I'm M1
I'm A1
I'm A2
I'm A3
M2: I'm A2
size = 1
dis = 28.2843
M1 destruct
```
### 2.装甲板分类
#### 分类原理

1. 用pytorch调用已经被广泛运用的resnet18，将数据集按照训练集：验证集=8：1进行训练。
2. 在RTX3060上训练15分钟，验证集上的准确率已达100%，这时训练集上准确率为97%。
3. 保存训练好的模型为ArmoClassificacion.pth,大小为134MB

#### 改进

1. 数据集中的样本过于单调，导致了过拟合现象，模型在网上下载的随机图片上的识别表现并不好。
2. 模型还是比较大，resnet18中包含了两位数个卷积层，偏复杂。可以试试魔改resnet18，减少几个层。

#### 环境

+ 基于torch2.0.1+cuda11.7+torchvision0.15.2

#### 训练图像预处理：

+ 训练集随机旋转45°，增加亮度至150%，每个像素点以均值和方差均为0.5进行归一化。
+ 验证集只做每个像素点均值和方差均为0.5的归一化。
+ 注意，ToTensor()这步操作会自动把像素的rgb值都除以255。

```python
data_transfroms = { 
    'train':transforms.Compose([
        transforms.RandomRotation(45),##随机转45°
        #transforms.CenterCrop(),##中心剪裁
        ##transforms.RandomHorizontalFlip(p=0.5),##随机水平镜像
        ##transforms.RandomVerticalFlip(p=0.5),##随机垂直镜像
        ##transforms.ColorJitter(brightness=1.5,contrast=1),
        transforms.ToTensor(),
        transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])
    ]),
    'valid':transforms.Compose([
        #transforms.Resize(256),##调整大小
        #transforms.CenterCrop(80),##中心剪裁
        transforms.ToTensor(),
        transforms.Normalize([0.5,0.5,0.5],[0.5,0.5,0.5])
    ])

}
```

+ 训练集图片
+ ![训练集图片](https://github.com/b-Acid/Images/blob/main/train.png?raw=true)



#### 网络初始化

+ 本处调用torchvision从官网下载并搭建了了resnet18。由于本任务是6分类任务，将resnet18的最后一层变为512*6的全连接层，并添加一层softmax层来得到输出类别序号。

```python
ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (relu): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): BasicBlock(
      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
   ...
   ...
   ...
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
  (fc): Sequential(
    (0): Linear(in_features=512, out_features=6, bias=True)
    (1): LogSoftmax(dim=1)
  )
)
```


#### 模型调用

+ 图像预处理
+ 神经网络接受3 * 80 * 80且已经归一化的矩阵，所以要对图像预处理，预处理函数如下：

```python
def process_image(image_path):
    img=Image.open(image_path)

    img=img.resize((80,80))

    img=np.array(img)/255

    mean=np.array([0.5,0.5,0.5])

    std=np.array([0.5,0.5,0.5])

    img=(img-mean)/std

    img=img.transpose((2,0,1))

    return img
```

+ 模型调用

```python
filename="3.png"
image1=process_image(filename)


output = model_ft(torch.unsqueeze(torch.FloatTensor(image1),0).to(device))
_,preds_tensor=torch.max(output,1)
preds=np.squeeze(preds_tensor.cpu().numpy())
flower_names[str(preds)]

print("Prediction:"+str(preds)+",True:"+filename[0])
imshow(image1,title="PRED:{},TRUE:{}".format(preds,filename[0]))
```

![识别效果图](https://github.com/b-Acid/Images/blob/main/output.png?raw=true)



### 3.onnx模型部署
#### 基本思路：
+ 创建一个yolo类，属性包含所有网络需要的参数，功能包括网络的推理和一些图片处理。
#### 关键实现代码
1. Configuration类
   + 使用Configuration类来指定一些可修改参数，初始化网络
   
   ```c++
   struct Configuration
   {
   	public: 
   	float confThreshold; // Confidence threshold
   	float nmsThreshold;  // Non-maximum suppression threshold
   	float objThreshold;  //Object Confidence threshold
   	string modelpath; //模型路径
   }
   ```
   + 这个类指明了模型的路径和一些阈值。
    
2. 创建yolov5类
   
   ```c++
   //yolov5类
   class YOLOv5
   {
   public:
   	YOLOv5(Configuration config);
   	void detect(Mat& frame);
   private:
   	float confThreshold;
   	float nmsThreshold;
   	float objThreshold;//三个配置量，也就是三个阈值
   	int inpWidth;
   	int inpHeight;//输入长宽
   	int nout;//输出的数据帧长度，对象中心xy，长宽wh，全局置信度（是一个对象的置信度），每个类别的置信度
   	int num_proposal;//探测到的类box数
   	int num_classes;//所有类别，也就是下面这些
   	string classes[1] = {"face"};
    
   	vector<float> input_image_;	// 输入图片
   	void normalize_(Mat img);		// 归一化函数
   	void nms(vector<BoxInfo>& input_boxes);  //非极大值抑制函数
   	Mat resize_image(Mat srcimg, int *newh, int *neww, int *top, int *left);//resize图片为模型输入大小
    
   	Env env = Env(ORT_LOGGING_LEVEL_ERROR, "yolov5-6.1"); // 初始化环境
   	Session *ort_session = nullptr;    // 初始化Session指针
   	SessionOptions sessionOptions = SessionOptions();  //初始化Session对象用的配置类
   	vector<char*> input_names;  // 定义一个字符指针vector
   	vector<char*> output_names; // 定义一个字符指针vector
   	vector<vector<int64_t>> input_node_dims; // >=1 inputs维度 
   	vector<vector<int64_t>> output_node_dims; // >=1 outputs维度
   };
   ```
   + 这步定义了模型运行的基本配置，包括使用Configuration类来初始化YOLOv5类，注释里有详细标明。
3. session类的配置
   ```c++
   string model_path = config.modelpath;//模型路径
 
   //CUDA设置
   OrtCUDAProviderOptions cuda_options{
          0,
          OrtCudnnConvAlgoSearch::EXHAUSTIVE,
          std::numeric_limits<size_t>::max()/10,
          0,
          true
      };
   sessionOptions.AppendExecutionProvider_CUDA(cuda_options);
   sessionOptions.SetGraphOptimizationLevel(ORT_ENABLE_BASIC);  //设置图优化类型
   ort_session = new Session(env, (const char*)model_path.c_str(), sessionOptions);//应用设置
   ```
4. 推理过程
   ```c++
	array<int64_t, 4> input_shape_{ 1, 3, this->inpHeight, this->inpWidth };
	
	//创建输入tensor
	auto allocator_info = MemoryInfo::CreateCpu(OrtDeviceAllocator, OrtMemTypeCPU);
	Value input_tensor_ = Value::CreateTensor<float>(allocator_info,
	    input_image_.data(), input_image_.size(), input_shape_.data(), input_shape_.size());
	
	// 开始推理
	vector<Value> ort_outputs = ort_session->Run(RunOptions{ nullptr }, &input_names[0], &input_tensor_, 1, output_names.data(), output_names.size());   // 开始推理
	float* pdata = ort_outputs[0].GetTensorMutableData<float>(); //输出流的头指针
   ```
   + 这步便得到了输出的数组了，在yolov5s原生模型中ort_outputs[0]是 1 * 25000 * 85  大小，在我的人脸识别模型中则为 1 * 6000 * 6 大小。第二维是所有预测的bounding-box的数量，第三维前两项是位置，  第三第四项是大小，第五项是置信度，后面是目标属于每类的分数。
   + 这里的pdata是一维float数组，一定要按照低维到高纬的顺序读取。读取到了数据后就可以非极大值抑制并画框了。
5. 效果图
    
![](https://github.com/b-Acid/24-vision-lwh/blob/main/onnx%E9%83%A8%E7%BD%B2/output.png?raw=true)

#### 使用同样的方式可以部署任务1中训练得到的resnet，将输出维度改为1*6即可。

### 4.tensorRT部署神经网络
#### 基本思路
+ 与onnxruntime部署方式基本一致，只是神经网络实现的方式不同。
#### 具体实现
1. 载入engine引擎：
```c++
char *trtModelStream{ nullptr }; //trtModelStream的指针
size_t size{ 0 }; 

std::ifstream file("face.engine", std::ios::binary); 
if (file.good()) { 
        file.seekg(0, file.end); //运动到尾指针
        size = file.tellg(); //获取当前指针，也就得到了文件大小
        file.seekg(0, file.beg); //移动到头指针
        trtModelStream = new char[size]; //创建同样大小的文件流，初始化trtModelStream（char的大小是1byte，其实就是以二进制格式写入）
        assert(trtModelStream); //检查点，创建文件流失败则退出
        file.read(trtModelStream, size); //读取
        file.close(); //关闭
} 
```
+ 这里把二进制格式的.engine文件读入二进制文件流trtModelStream，将在下步载入ICudaEngine类中。


2. 创建createInferRuntime
```
Logger m_logger; 
IRuntime* runtime = createInferRuntime(m_logger); 
ICudaEngine* engine = runtime->deserializeCudaEngine(trtModelStream, size, nullptr); 
IExecutionContext* context = engine->createExecutionContext(); 

```
+ 这里ILogger类是抽象类，必须实现一个子类继承它来实例化，并且子类要求重写log函数。其实是个报告信息用的类，不想打印提示信息可以随便写。这里实现了一个子类Logger来实例化对象。
+ 之后创建一个runtime，顾名思义运行状态，它由m_logger报告信息。
+ 接着创建推理引擎，也就是把刚刚的二进制文件流trtModelStream写进来。
+ 创建一个IExecutionContext，直译是当前线程的上下文，作用是开一个线程。一个engine可以有多个executioncontext，并允许将同一套weights用于多个推理任务。
3. 分配显存
```c++
cudaMalloc(&buffers[inputIndex], batchSize * CHANNEL * IN_H * IN_W * sizeof(float)); 
cudaMalloc(&buffers[outputIndex], OUT * sizeof(float)); 
```
+ 这里分配了输入和输出数据的缓存区。

4. 搬数据到显存上并推理
```c++
// Create stream 
cudaStream_t stream; 
cudaStreamCreate(&stream); 

// DMA input batch data to device, infer on the batch asynchronously, and DMA output back to host 
cudaMemcpyAsync(buffers[inputIndex], input, batchSize * CHANNEL * IN_H * IN_W * sizeof(float), cudaMemcpyHostToDevice, stream); 
context.enqueue(batchSize, buffers, stream, nullptr); 
cudaMemcpyAsync(output, buffers[outputIndex], OUT * sizeof(float), cudaMemcpyDeviceToHost, stream); 
cudaStreamSynchronize(stream); 
```

+ 首先实例化一个cuda流，完成数据搬运功能。
+ 把输入数组搬到显存上。
+ 推理，这时输出数据被保存在buffer[1]里。
+ 把输出数据搬回内参上给cpu进一步操作。

5. 后续处理
+ 现在输出数据保存在output里，按照face.engine，它的大小为1 * 3600 * 6。现在可以依次读出3600个boudingbox的6个数据，进行非极大值抑制，然后完成绘图。

6. 效果图
+ ![](https://github.com/b-Acid/24-vision-lwh/blob/main/tensorRT%E9%83%A8%E7%BD%B2/out-1.png?raw=true)


### 5.点云投影任务
#### 基本原理 
+ 相机外参是一个4×4矩阵
  
$$
CameraExtrinsicMat=\left[
 \begin{matrix}
    --0.00719 & 0.0104 & 1 & 0.1 \\
   -1 & -0.00225 & -0.00721 & -0.0182 \\
  -0.00223 & -1 & -0.0104 & -0.173\\
   0 & 0 & 0 & 1 
  \end{matrix}
  \right]
$$

+ 相机内参是一个3×3矩阵

$$
CameraMat=\left[
 \begin{matrix}
  1385 & 0 & 936\\
  0 &  1385 & 494\\
   0 &  0 & 1 
  \end{matrix}
  \right] 
$$

+ 世界坐标下的点 $[Xw,Yw,Zw]$ 扩维后化为列向量 $[Xw,Yw,Zw,1]^T $,左乘外参矩阵 $CameraExtrinsicMat$ 得到相机坐标 $[Xc,Yc,Zc,1]$ 。
+ 相机坐标 $[Xc,Yc,Zc]$ 左乘内参矩阵 $CameraMat$ 再除以z轴深度Zc得到像素矩阵 $[u,v,1]$ 。
``` python
for i in range(data.shape[0]):#data是n*4的矩阵，即所有点的世界坐标数据扩大一维。
    temp=(data[i,:]).T#点坐标转列向量
    point=np.matmul(temp,CameraExtrinsicMat)#矩阵计算相机坐标
    DISTANCE[i]=mt.sqrt(point[0]**2+point[1]**2+point[2]**2)#计算深度
    point=np.matmul(point[0:3],CameraMat)/point[2]#矩阵计算像素坐标
    POINTS[i,:]=point[0:2]#保存像素坐标
```
+ 筛选位于视野内的点，保存所有信息至PixPoint，它是一个n*3的向量，表示像素坐标（2）+深度信息（1）。
+ 依据深度信息对不同的世界点描绘不同大小的圆，越深的点描更大的圆。这里使用的是opencv里的circle，最远的点画半径为3的实心圆，最近的点画半径为0的实心圆（像素点）。
```python
for i in range(PixPoint.shape[0]):
    if PixPoint[i,2]<mind+cut:
        size=0
    if mind+cut<=PixPoint[i,2]<mind+2*cut:
        size=1
    if mind+2*cut<=PixPoint[i,2]<mind+3*cut:
        size=2
    if mind+3*cut<=PixPoint[i,2]:
        size=3
    cv2.circle(img, (int(PixPoint[i,0]+w/2),int(PixPoint[i,1]+h/2)), size, (255, 255, 255), -1)#画点，其实就是实心圆
```
+ 
  
#### 效果：
![](https://github.com/b-Acid/24-vision-lwh/blob/main/%E7%82%B9%E4%BA%91%E6%8A%95%E5%BD%B1%E4%BB%BB%E5%8A%A1/outputs/one/cloud_2.jpg?raw=true)

## 三、总结
+ 学会了使用pytorch搭建，训练，运行神经网络。
+ 学会了使用onnxruntime简单部署一个神经网络。
+ 学会了使用tensorRT简单部署一个神经网络。
+ 学会了世界坐标，相机坐标，像素坐标之间的转化，学会了相机内外参矩阵的含义。



运行效果地址：https://github.com/b-Acid/24-vision-lwh

git仓库地址：https://github.com/b-Acid/24-vision-lwh
